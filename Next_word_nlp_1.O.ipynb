{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Next Word Prediction"
      ],
      "metadata": {
        "id": "9GBfVTV9_cg9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the requeired libraries"
      ],
      "metadata": {
        "id": "oZeiWojE_lMG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8n58Tp8Qgtjx"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pickle\n",
        "import numpy as np\n",
        "import string\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "reading the text file"
      ],
      "metadata": {
        "id": "qTlz0JOo_oln"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CURhCpOh8-4",
        "outputId": "cefc89a4-ae86-4866-f143-b21f79b40f94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking the lines :\n",
            "ï»¿One morning, when Gregor Samsa woke from troubled dreams, he found\n",
            "\n",
            "first to get up and stretch out her young body.\n"
          ]
        }
      ],
      "source": [
        "file = open(\"/content/clean.txt\", \"r\", encoding = \"utf8\")\n",
        "lines = []\n",
        "\n",
        "for i in file:\n",
        "    lines.append(i)\n",
        "\n",
        "print(\"Checking the lines :\")\n",
        "    \n",
        "print(lines[0])\n",
        "print(lines[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "some preprocessing steps"
      ],
      "metadata": {
        "id": "RpK7UC5a_rPR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "-4MwgTeUiMTH",
        "outputId": "c0dc94b7-02a6-498c-97c0-3e57ed86f252"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.  He lay on his armour-like back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections.  The bedding was hardly able to cover it and seemed ready to slide off any moment.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "data = \"\"\n",
        "\n",
        "for i in lines:\n",
        "    data = ' '. join(lines)\n",
        "    \n",
        "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
        "data[:360]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "JnVxLxjzibS9",
        "outputId": "461527fd-c931-4119-c072-022acf63598f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'One morning  when Gregor Samsa woke from troubled dreams  he found himself transformed in his bed into a horrible vermin   He lay on his armour like back  and if he lifted his head a little he could see his brown belly  slightly domed and divided by arches into stiff sections   The bedding was hardly able to cover it and seemed ready to slide off any moment   His many legs  pitifully thin compared with the size of the rest of him  waved about helplessly as he looked    What s happened to me   he'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
        "new_data = data.translate(translator)\n",
        "\n",
        "new_data[:500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Is00nTTNi2Gg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "2c2dd4bf-4f77-4bd1-c1aa-d4ac10470939"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. He lay on armour-like back, and if lifted head little could see brown belly, slightly domed divided by arches stiff sections. The bedding was hardly able to cover it seemed ready slide off any moment. His many legs, pitifully thin compared with the size of rest him, waved about helplessly as looked. \"What\\'s happened me?\" thought. It wasn\\'t dream. room, proper human room altho'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "z = []\n",
        "\n",
        "for i in data.split():\n",
        "    if i not in z:\n",
        "        z.append(i)\n",
        "        \n",
        "data = ' '.join(z)\n",
        "data[:500]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization"
      ],
      "metadata": {
        "id": "joWQaSld_uO-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpktDQL4i2DQ",
        "outputId": "4a766844-627e-4842-ea6a-e043cf1cd559"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[17, 53, 293, 2, 18, 729, 135, 730, 294, 8]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "\n",
        "# saving the tokenizer for predict function.\n",
        "pickle.dump(tokenizer, open('tokenizer1.pkl', 'wb'))\n",
        "\n",
        "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
        "sequence_data[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBdwqG3pi2Ax",
        "outputId": "ca06a839-15f7-4a3b-e0d0-210643763848"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2617\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4G9aT3PAi1-E",
        "outputId": "b428f5a9-e61c-4842-cad7-88298ec3624e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Length of sequences are:  3889\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 17,  53],\n",
              "       [ 53, 293],\n",
              "       [293,   2],\n",
              "       [  2,  18],\n",
              "       [ 18, 729],\n",
              "       [729, 135],\n",
              "       [135, 730],\n",
              "       [730, 294],\n",
              "       [294,   8],\n",
              "       [  8, 731]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "sequences = []\n",
        "\n",
        "for i in range(1, len(sequence_data)):\n",
        "    words = sequence_data[i-1:i+1]\n",
        "    sequences.append(words)\n",
        "    \n",
        "print(\"The Length of sequences are: \", len(sequences))\n",
        "sequences = np.array(sequences)\n",
        "sequences[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "awmFYL7Hi17e"
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in sequences:\n",
        "    X.append(i[0])\n",
        "    y.append(i[1])\n",
        "    \n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyZoZHXfi145",
        "outputId": "9f485917-d8be-41c5-d723-7242bd037709"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first words:  [ 17  53 293   2  18]\n",
            "The next connect words:  [ 53 293   2  18 729]\n"
          ]
        }
      ],
      "source": [
        "print(\"The first words: \", X[:5])\n",
        "print(\"The next connect words: \", y[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5sf0kXbi12V",
        "outputId": "18679dd3-5fca-42f5-d1dc-a6999c1e0ad8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "y[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8DylQEji1zw",
        "outputId": "1b3be6b6-1fbc-4d2c-e2f6-287482833090"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2617"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "vocab_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCjfXPacjdup"
      },
      "source": [
        "Creating Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yvP5-zQ4i1xH"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=1))\n",
        "model.add(LSTM(1000, return_sequences=True))\n",
        "model.add(LSTM(1000))\n",
        "model.add(Dense(1000, activation=\"relu\"))\n",
        "model.add(Dense(vocab_size, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUyYlNO4ipJf",
        "outputId": "b553decb-ba83-44ea-9e47-291393fdbae2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 1, 10)             26170     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 1, 1000)           4044000   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1000)              1001000   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2617)              2619617   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15,694,787\n",
            "Trainable params: 15,694,787\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "JI2eq9GPjs7v",
        "outputId": "c3e613ef-8b50-46b4-ca44-e24c5beccf33"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Image object>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "keras.utils.plot_model(model, to_file='model.png', show_layer_names=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "XCDHJf_aj6YP"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1,\n",
        "    save_best_only=True, mode='auto')\n",
        "\n",
        "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
        "\n",
        "logdir='logsnextword1'\n",
        "tensorboard_Visualization = TensorBoard(log_dir=logdir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJ16rJtFkB4M",
        "outputId": "cd2d2137-64cc-4dc1-c7a5-aa500ff35e3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001) ,metrics = [\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEvfXXtPkFvT",
        "outputId": "0135f8ba-fefe-4809-d4e9-f4a22e545944"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.8754 - accuracy: 0.0013\n",
            "Epoch 1: loss improved from inf to 7.87541, saving model to nextword1.h5\n",
            "61/61 [==============================] - 18s 214ms/step - loss: 7.8754 - accuracy: 0.0013 - lr: 0.0010\n",
            "Epoch 2/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.8616 - accuracy: 0.0026\n",
            "Epoch 2: loss improved from 7.87541 to 7.86160, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 214ms/step - loss: 7.8616 - accuracy: 0.0026 - lr: 0.0010\n",
            "Epoch 3/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.8176 - accuracy: 0.0041\n",
            "Epoch 3: loss improved from 7.86160 to 7.81758, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 214ms/step - loss: 7.8176 - accuracy: 0.0041 - lr: 0.0010\n",
            "Epoch 4/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.6930 - accuracy: 0.0036\n",
            "Epoch 4: loss improved from 7.81758 to 7.69302, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 215ms/step - loss: 7.6930 - accuracy: 0.0036 - lr: 0.0010\n",
            "Epoch 5/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.4879 - accuracy: 0.0018\n",
            "Epoch 5: loss improved from 7.69302 to 7.48785, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 214ms/step - loss: 7.4879 - accuracy: 0.0018 - lr: 0.0010\n",
            "Epoch 6/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.3074 - accuracy: 0.0023\n",
            "Epoch 6: loss improved from 7.48785 to 7.30738, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 213ms/step - loss: 7.3074 - accuracy: 0.0023 - lr: 0.0010\n",
            "Epoch 7/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.1793 - accuracy: 0.0023\n",
            "Epoch 7: loss improved from 7.30738 to 7.17929, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 210ms/step - loss: 7.1793 - accuracy: 0.0023 - lr: 0.0010\n",
            "Epoch 8/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.0634 - accuracy: 0.0023\n",
            "Epoch 8: loss improved from 7.17929 to 7.06345, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 213ms/step - loss: 7.0634 - accuracy: 0.0023 - lr: 0.0010\n",
            "Epoch 9/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.9202 - accuracy: 0.0036\n",
            "Epoch 9: loss improved from 7.06345 to 6.92021, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 213ms/step - loss: 6.9202 - accuracy: 0.0036 - lr: 0.0010\n",
            "Epoch 10/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.7367 - accuracy: 0.0051\n",
            "Epoch 10: loss improved from 6.92021 to 6.73670, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 214ms/step - loss: 6.7367 - accuracy: 0.0051 - lr: 0.0010\n",
            "Epoch 11/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.5264 - accuracy: 0.0039\n",
            "Epoch 11: loss improved from 6.73670 to 6.52639, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 215ms/step - loss: 6.5264 - accuracy: 0.0039 - lr: 0.0010\n",
            "Epoch 12/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.3008 - accuracy: 0.0082\n",
            "Epoch 12: loss improved from 6.52639 to 6.30077, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 215ms/step - loss: 6.3008 - accuracy: 0.0082 - lr: 0.0010\n",
            "Epoch 13/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.0738 - accuracy: 0.0118\n",
            "Epoch 13: loss improved from 6.30077 to 6.07381, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 6.0738 - accuracy: 0.0118 - lr: 0.0010\n",
            "Epoch 14/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.8917 - accuracy: 0.0103\n",
            "Epoch 14: loss improved from 6.07381 to 5.89168, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 214ms/step - loss: 5.8917 - accuracy: 0.0103 - lr: 0.0010\n",
            "Epoch 15/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.6940 - accuracy: 0.0134\n",
            "Epoch 15: loss improved from 5.89168 to 5.69400, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 215ms/step - loss: 5.6940 - accuracy: 0.0134 - lr: 0.0010\n",
            "Epoch 16/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.5446 - accuracy: 0.0172\n",
            "Epoch 16: loss improved from 5.69400 to 5.54460, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 214ms/step - loss: 5.5446 - accuracy: 0.0172 - lr: 0.0010\n",
            "Epoch 17/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.3804 - accuracy: 0.0195\n",
            "Epoch 17: loss improved from 5.54460 to 5.38041, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 214ms/step - loss: 5.3804 - accuracy: 0.0195 - lr: 0.0010\n",
            "Epoch 18/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.2460 - accuracy: 0.0221\n",
            "Epoch 18: loss improved from 5.38041 to 5.24604, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 214ms/step - loss: 5.2460 - accuracy: 0.0221 - lr: 0.0010\n",
            "Epoch 19/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.1236 - accuracy: 0.0265\n",
            "Epoch 19: loss improved from 5.24604 to 5.12362, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 213ms/step - loss: 5.1236 - accuracy: 0.0265 - lr: 0.0010\n",
            "Epoch 20/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.9923 - accuracy: 0.0283\n",
            "Epoch 20: loss improved from 5.12362 to 4.99228, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 4.9923 - accuracy: 0.0283 - lr: 0.0010\n",
            "Epoch 21/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.8679 - accuracy: 0.0345\n",
            "Epoch 21: loss improved from 4.99228 to 4.86788, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 215ms/step - loss: 4.8679 - accuracy: 0.0345 - lr: 0.0010\n",
            "Epoch 22/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.7846 - accuracy: 0.0324\n",
            "Epoch 22: loss improved from 4.86788 to 4.78458, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 215ms/step - loss: 4.7846 - accuracy: 0.0324 - lr: 0.0010\n",
            "Epoch 23/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.6745 - accuracy: 0.0383\n",
            "Epoch 23: loss improved from 4.78458 to 4.67447, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 4.6745 - accuracy: 0.0383 - lr: 0.0010\n",
            "Epoch 24/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.5950 - accuracy: 0.0442\n",
            "Epoch 24: loss improved from 4.67447 to 4.59502, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 4.5950 - accuracy: 0.0442 - lr: 0.0010\n",
            "Epoch 25/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.5215 - accuracy: 0.0463\n",
            "Epoch 25: loss improved from 4.59502 to 4.52152, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 4.5215 - accuracy: 0.0463 - lr: 0.0010\n",
            "Epoch 26/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.4476 - accuracy: 0.0489\n",
            "Epoch 26: loss improved from 4.52152 to 4.44765, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 215ms/step - loss: 4.4476 - accuracy: 0.0489 - lr: 0.0010\n",
            "Epoch 27/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.3615 - accuracy: 0.0486\n",
            "Epoch 27: loss improved from 4.44765 to 4.36154, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 4.3615 - accuracy: 0.0486 - lr: 0.0010\n",
            "Epoch 28/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.3009 - accuracy: 0.0522\n",
            "Epoch 28: loss improved from 4.36154 to 4.30088, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 4.3009 - accuracy: 0.0522 - lr: 0.0010\n",
            "Epoch 29/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.2315 - accuracy: 0.0576\n",
            "Epoch 29: loss improved from 4.30088 to 4.23148, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 232ms/step - loss: 4.2315 - accuracy: 0.0576 - lr: 0.0010\n",
            "Epoch 30/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.1450 - accuracy: 0.0643\n",
            "Epoch 30: loss improved from 4.23148 to 4.14504, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 232ms/step - loss: 4.1450 - accuracy: 0.0643 - lr: 0.0010\n",
            "Epoch 31/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.0553 - accuracy: 0.0771\n",
            "Epoch 31: loss improved from 4.14504 to 4.05533, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 4.0553 - accuracy: 0.0771 - lr: 0.0010\n",
            "Epoch 32/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.9496 - accuracy: 0.0818\n",
            "Epoch 32: loss improved from 4.05533 to 3.94965, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 219ms/step - loss: 3.9496 - accuracy: 0.0818 - lr: 0.0010\n",
            "Epoch 33/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.8394 - accuracy: 0.0892\n",
            "Epoch 33: loss improved from 3.94965 to 3.83939, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 221ms/step - loss: 3.8394 - accuracy: 0.0892 - lr: 0.0010\n",
            "Epoch 34/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.7386 - accuracy: 0.0975\n",
            "Epoch 34: loss improved from 3.83939 to 3.73860, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 217ms/step - loss: 3.7386 - accuracy: 0.0975 - lr: 0.0010\n",
            "Epoch 35/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.6526 - accuracy: 0.1155\n",
            "Epoch 35: loss improved from 3.73860 to 3.65258, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 3.6526 - accuracy: 0.1155 - lr: 0.0010\n",
            "Epoch 36/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.5084 - accuracy: 0.1227\n",
            "Epoch 36: loss improved from 3.65258 to 3.50836, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 3.5084 - accuracy: 0.1227 - lr: 0.0010\n",
            "Epoch 37/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.3999 - accuracy: 0.1399\n",
            "Epoch 37: loss improved from 3.50836 to 3.39993, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 215ms/step - loss: 3.3999 - accuracy: 0.1399 - lr: 0.0010\n",
            "Epoch 38/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.3129 - accuracy: 0.1481\n",
            "Epoch 38: loss improved from 3.39993 to 3.31287, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 215ms/step - loss: 3.3129 - accuracy: 0.1481 - lr: 0.0010\n",
            "Epoch 39/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.2211 - accuracy: 0.1587\n",
            "Epoch 39: loss improved from 3.31287 to 3.22111, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 215ms/step - loss: 3.2211 - accuracy: 0.1587 - lr: 0.0010\n",
            "Epoch 40/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.1172 - accuracy: 0.1617\n",
            "Epoch 40: loss improved from 3.22111 to 3.11719, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 215ms/step - loss: 3.1172 - accuracy: 0.1617 - lr: 0.0010\n",
            "Epoch 41/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.0264 - accuracy: 0.1849\n",
            "Epoch 41: loss improved from 3.11719 to 3.02638, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 213ms/step - loss: 3.0264 - accuracy: 0.1849 - lr: 0.0010\n",
            "Epoch 42/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.9702 - accuracy: 0.1962\n",
            "Epoch 42: loss improved from 3.02638 to 2.97017, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 215ms/step - loss: 2.9702 - accuracy: 0.1962 - lr: 0.0010\n",
            "Epoch 43/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.8830 - accuracy: 0.2031\n",
            "Epoch 43: loss improved from 2.97017 to 2.88296, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 215ms/step - loss: 2.8830 - accuracy: 0.2031 - lr: 0.0010\n",
            "Epoch 44/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.8156 - accuracy: 0.2160\n",
            "Epoch 44: loss improved from 2.88296 to 2.81561, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 214ms/step - loss: 2.8156 - accuracy: 0.2160 - lr: 0.0010\n",
            "Epoch 45/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.7353 - accuracy: 0.2222\n",
            "Epoch 45: loss improved from 2.81561 to 2.73533, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 214ms/step - loss: 2.7353 - accuracy: 0.2222 - lr: 0.0010\n",
            "Epoch 46/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.6265 - accuracy: 0.2440\n",
            "Epoch 46: loss improved from 2.73533 to 2.62646, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 215ms/step - loss: 2.6265 - accuracy: 0.2440 - lr: 0.0010\n",
            "Epoch 47/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.5608 - accuracy: 0.2571\n",
            "Epoch 47: loss improved from 2.62646 to 2.56078, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 2.5608 - accuracy: 0.2571 - lr: 0.0010\n",
            "Epoch 48/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.4908 - accuracy: 0.2690\n",
            "Epoch 48: loss improved from 2.56078 to 2.49079, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 215ms/step - loss: 2.4908 - accuracy: 0.2690 - lr: 0.0010\n",
            "Epoch 49/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.3945 - accuracy: 0.2864\n",
            "Epoch 49: loss improved from 2.49079 to 2.39451, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 217ms/step - loss: 2.3945 - accuracy: 0.2864 - lr: 0.0010\n",
            "Epoch 50/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.3037 - accuracy: 0.3034\n",
            "Epoch 50: loss improved from 2.39451 to 2.30374, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 215ms/step - loss: 2.3037 - accuracy: 0.3034 - lr: 0.0010\n",
            "Epoch 51/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.2419 - accuracy: 0.3230\n",
            "Epoch 51: loss improved from 2.30374 to 2.24194, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 215ms/step - loss: 2.2419 - accuracy: 0.3230 - lr: 0.0010\n",
            "Epoch 52/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.1707 - accuracy: 0.3302\n",
            "Epoch 52: loss improved from 2.24194 to 2.17070, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 214ms/step - loss: 2.1707 - accuracy: 0.3302 - lr: 0.0010\n",
            "Epoch 53/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.1040 - accuracy: 0.3538\n",
            "Epoch 53: loss improved from 2.17070 to 2.10398, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 2.1040 - accuracy: 0.3538 - lr: 0.0010\n",
            "Epoch 54/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.0228 - accuracy: 0.3669\n",
            "Epoch 54: loss improved from 2.10398 to 2.02278, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 217ms/step - loss: 2.0228 - accuracy: 0.3669 - lr: 0.0010\n",
            "Epoch 55/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.9723 - accuracy: 0.3834\n",
            "Epoch 55: loss improved from 2.02278 to 1.97235, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 214ms/step - loss: 1.9723 - accuracy: 0.3834 - lr: 0.0010\n",
            "Epoch 56/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.9057 - accuracy: 0.3932\n",
            "Epoch 56: loss improved from 1.97235 to 1.90566, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 215ms/step - loss: 1.9057 - accuracy: 0.3932 - lr: 0.0010\n",
            "Epoch 57/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.8527 - accuracy: 0.4055\n",
            "Epoch 57: loss improved from 1.90566 to 1.85269, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 214ms/step - loss: 1.8527 - accuracy: 0.4055 - lr: 0.0010\n",
            "Epoch 58/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7872 - accuracy: 0.4153\n",
            "Epoch 58: loss improved from 1.85269 to 1.78716, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 214ms/step - loss: 1.7872 - accuracy: 0.4153 - lr: 0.0010\n",
            "Epoch 59/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7585 - accuracy: 0.4250\n",
            "Epoch 59: loss improved from 1.78716 to 1.75847, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 215ms/step - loss: 1.7585 - accuracy: 0.4250 - lr: 0.0010\n",
            "Epoch 60/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7011 - accuracy: 0.4371\n",
            "Epoch 60: loss improved from 1.75847 to 1.70115, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 1.7011 - accuracy: 0.4371 - lr: 0.0010\n",
            "Epoch 61/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6474 - accuracy: 0.4497\n",
            "Epoch 61: loss improved from 1.70115 to 1.64741, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 215ms/step - loss: 1.6474 - accuracy: 0.4497 - lr: 0.0010\n",
            "Epoch 62/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6073 - accuracy: 0.4726\n",
            "Epoch 62: loss improved from 1.64741 to 1.60726, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 214ms/step - loss: 1.6073 - accuracy: 0.4726 - lr: 0.0010\n",
            "Epoch 63/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5394 - accuracy: 0.4752\n",
            "Epoch 63: loss improved from 1.60726 to 1.53941, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 217ms/step - loss: 1.5394 - accuracy: 0.4752 - lr: 0.0010\n",
            "Epoch 64/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5129 - accuracy: 0.4893\n",
            "Epoch 64: loss improved from 1.53941 to 1.51290, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 214ms/step - loss: 1.5129 - accuracy: 0.4893 - lr: 0.0010\n",
            "Epoch 65/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4705 - accuracy: 0.4960\n",
            "Epoch 65: loss improved from 1.51290 to 1.47053, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 1.4705 - accuracy: 0.4960 - lr: 0.0010\n",
            "Epoch 66/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4251 - accuracy: 0.5040\n",
            "Epoch 66: loss improved from 1.47053 to 1.42514, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 215ms/step - loss: 1.4251 - accuracy: 0.5040 - lr: 0.0010\n",
            "Epoch 67/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3787 - accuracy: 0.5153\n",
            "Epoch 67: loss improved from 1.42514 to 1.37874, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 215ms/step - loss: 1.3787 - accuracy: 0.5153 - lr: 0.0010\n",
            "Epoch 68/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3717 - accuracy: 0.5184\n",
            "Epoch 68: loss improved from 1.37874 to 1.37168, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 1.3717 - accuracy: 0.5184 - lr: 0.0010\n",
            "Epoch 69/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3350 - accuracy: 0.5351\n",
            "Epoch 69: loss improved from 1.37168 to 1.33497, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 1.3350 - accuracy: 0.5351 - lr: 0.0010\n",
            "Epoch 70/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2973 - accuracy: 0.5343\n",
            "Epoch 70: loss improved from 1.33497 to 1.29730, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 1.2973 - accuracy: 0.5343 - lr: 0.0010\n",
            "Epoch 71/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2903 - accuracy: 0.5372\n",
            "Epoch 71: loss improved from 1.29730 to 1.29030, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 1.2903 - accuracy: 0.5372 - lr: 0.0010\n",
            "Epoch 72/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2643 - accuracy: 0.5418\n",
            "Epoch 72: loss improved from 1.29030 to 1.26429, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 1.2643 - accuracy: 0.5418 - lr: 0.0010\n",
            "Epoch 73/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2535 - accuracy: 0.5474\n",
            "Epoch 73: loss improved from 1.26429 to 1.25352, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 217ms/step - loss: 1.2535 - accuracy: 0.5474 - lr: 0.0010\n",
            "Epoch 74/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2412 - accuracy: 0.5467\n",
            "Epoch 74: loss improved from 1.25352 to 1.24122, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 217ms/step - loss: 1.2412 - accuracy: 0.5467 - lr: 0.0010\n",
            "Epoch 75/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2388 - accuracy: 0.5397\n",
            "Epoch 75: loss improved from 1.24122 to 1.23885, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 227ms/step - loss: 1.2388 - accuracy: 0.5397 - lr: 0.0010\n",
            "Epoch 76/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2127 - accuracy: 0.5397\n",
            "Epoch 76: loss improved from 1.23885 to 1.21273, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 264ms/step - loss: 1.2127 - accuracy: 0.5397 - lr: 0.0010\n",
            "Epoch 77/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1911 - accuracy: 0.5539\n",
            "Epoch 77: loss improved from 1.21273 to 1.19106, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 1.1911 - accuracy: 0.5539 - lr: 0.0010\n",
            "Epoch 78/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1769 - accuracy: 0.5508\n",
            "Epoch 78: loss improved from 1.19106 to 1.17688, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 1.1769 - accuracy: 0.5508 - lr: 0.0010\n",
            "Epoch 79/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1484 - accuracy: 0.5634\n",
            "Epoch 79: loss improved from 1.17688 to 1.14844, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 1.1484 - accuracy: 0.5634 - lr: 0.0010\n",
            "Epoch 80/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1369 - accuracy: 0.5513\n",
            "Epoch 80: loss improved from 1.14844 to 1.13689, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 1.1369 - accuracy: 0.5513 - lr: 0.0010\n",
            "Epoch 81/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1257 - accuracy: 0.5608\n",
            "Epoch 81: loss improved from 1.13689 to 1.12566, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 219ms/step - loss: 1.1257 - accuracy: 0.5608 - lr: 0.0010\n",
            "Epoch 82/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1100 - accuracy: 0.5690\n",
            "Epoch 82: loss improved from 1.12566 to 1.11001, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 1.1100 - accuracy: 0.5690 - lr: 0.0010\n",
            "Epoch 83/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1122 - accuracy: 0.5618\n",
            "Epoch 83: loss did not improve from 1.11001\n",
            "61/61 [==============================] - 13s 209ms/step - loss: 1.1122 - accuracy: 0.5618 - lr: 0.0010\n",
            "Epoch 84/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1092 - accuracy: 0.5667\n",
            "Epoch 84: loss improved from 1.11001 to 1.10920, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 217ms/step - loss: 1.1092 - accuracy: 0.5667 - lr: 0.0010\n",
            "Epoch 85/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1125 - accuracy: 0.5683\n",
            "Epoch 85: loss did not improve from 1.10920\n",
            "61/61 [==============================] - 13s 208ms/step - loss: 1.1125 - accuracy: 0.5683 - lr: 0.0010\n",
            "Epoch 86/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1094 - accuracy: 0.5611\n",
            "Epoch 86: loss did not improve from 1.10920\n",
            "61/61 [==============================] - 13s 208ms/step - loss: 1.1094 - accuracy: 0.5611 - lr: 0.0010\n",
            "Epoch 87/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0974 - accuracy: 0.5621\n",
            "Epoch 87: loss improved from 1.10920 to 1.09737, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 1.0974 - accuracy: 0.5621 - lr: 0.0010\n",
            "Epoch 88/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0930 - accuracy: 0.5654\n",
            "Epoch 88: loss improved from 1.09737 to 1.09301, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 1.0930 - accuracy: 0.5654 - lr: 0.0010\n",
            "Epoch 89/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0991 - accuracy: 0.5562\n",
            "Epoch 89: loss did not improve from 1.09301\n",
            "61/61 [==============================] - 13s 209ms/step - loss: 1.0991 - accuracy: 0.5562 - lr: 0.0010\n",
            "Epoch 90/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0996 - accuracy: 0.5595\n",
            "Epoch 90: loss did not improve from 1.09301\n",
            "61/61 [==============================] - 13s 209ms/step - loss: 1.0996 - accuracy: 0.5595 - lr: 0.0010\n",
            "Epoch 91/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0913 - accuracy: 0.5521\n",
            "Epoch 91: loss improved from 1.09301 to 1.09126, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 1.0913 - accuracy: 0.5521 - lr: 0.0010\n",
            "Epoch 92/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0919 - accuracy: 0.5534\n",
            "Epoch 92: loss did not improve from 1.09126\n",
            "61/61 [==============================] - 13s 209ms/step - loss: 1.0919 - accuracy: 0.5534 - lr: 0.0010\n",
            "Epoch 93/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0958 - accuracy: 0.5549\n",
            "Epoch 93: loss did not improve from 1.09126\n",
            "61/61 [==============================] - 13s 210ms/step - loss: 1.0958 - accuracy: 0.5549 - lr: 0.0010\n",
            "Epoch 94/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0876 - accuracy: 0.5528\n",
            "Epoch 94: loss improved from 1.09126 to 1.08757, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 217ms/step - loss: 1.0876 - accuracy: 0.5528 - lr: 0.0010\n",
            "Epoch 95/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0707 - accuracy: 0.5536\n",
            "Epoch 95: loss improved from 1.08757 to 1.07071, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 214ms/step - loss: 1.0707 - accuracy: 0.5536 - lr: 0.0010\n",
            "Epoch 96/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0415 - accuracy: 0.5552\n",
            "Epoch 96: loss improved from 1.07071 to 1.04146, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 1.0415 - accuracy: 0.5552 - lr: 0.0010\n",
            "Epoch 97/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0333 - accuracy: 0.5598\n",
            "Epoch 97: loss improved from 1.04146 to 1.03327, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 1.0333 - accuracy: 0.5598 - lr: 0.0010\n",
            "Epoch 98/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0311 - accuracy: 0.5618\n",
            "Epoch 98: loss improved from 1.03327 to 1.03109, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 1.0311 - accuracy: 0.5618 - lr: 0.0010\n",
            "Epoch 99/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.9970 - accuracy: 0.5719\n",
            "Epoch 99: loss improved from 1.03109 to 0.99703, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 215ms/step - loss: 0.9970 - accuracy: 0.5719 - lr: 0.0010\n",
            "Epoch 100/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0180 - accuracy: 0.5629\n",
            "Epoch 100: loss did not improve from 0.99703\n",
            "61/61 [==============================] - 13s 208ms/step - loss: 1.0180 - accuracy: 0.5629 - lr: 0.0010\n",
            "Epoch 101/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.9881 - accuracy: 0.5706\n",
            "Epoch 101: loss improved from 0.99703 to 0.98812, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 217ms/step - loss: 0.9881 - accuracy: 0.5706 - lr: 0.0010\n",
            "Epoch 102/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.9889 - accuracy: 0.5662\n",
            "Epoch 102: loss did not improve from 0.98812\n",
            "61/61 [==============================] - 13s 209ms/step - loss: 0.9889 - accuracy: 0.5662 - lr: 0.0010\n",
            "Epoch 103/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.9769 - accuracy: 0.5631\n",
            "Epoch 103: loss improved from 0.98812 to 0.97694, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 217ms/step - loss: 0.9769 - accuracy: 0.5631 - lr: 0.0010\n",
            "Epoch 104/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.9746 - accuracy: 0.5711\n",
            "Epoch 104: loss improved from 0.97694 to 0.97459, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 217ms/step - loss: 0.9746 - accuracy: 0.5711 - lr: 0.0010\n",
            "Epoch 105/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.9670 - accuracy: 0.5711\n",
            "Epoch 105: loss improved from 0.97459 to 0.96702, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 0.9670 - accuracy: 0.5711 - lr: 0.0010\n",
            "Epoch 106/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.9621 - accuracy: 0.5647\n",
            "Epoch 106: loss improved from 0.96702 to 0.96208, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 219ms/step - loss: 0.9621 - accuracy: 0.5647 - lr: 0.0010\n",
            "Epoch 107/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.9568 - accuracy: 0.5678\n",
            "Epoch 107: loss improved from 0.96208 to 0.95676, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 0.9568 - accuracy: 0.5678 - lr: 0.0010\n",
            "Epoch 108/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.9404 - accuracy: 0.5690\n",
            "Epoch 108: loss improved from 0.95676 to 0.94040, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 217ms/step - loss: 0.9404 - accuracy: 0.5690 - lr: 0.0010\n",
            "Epoch 109/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.9430 - accuracy: 0.5752\n",
            "Epoch 109: loss did not improve from 0.94040\n",
            "61/61 [==============================] - 13s 208ms/step - loss: 0.9430 - accuracy: 0.5752 - lr: 0.0010\n",
            "Epoch 110/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.9426 - accuracy: 0.5672\n",
            "Epoch 110: loss did not improve from 0.94040\n",
            "61/61 [==============================] - 13s 208ms/step - loss: 0.9426 - accuracy: 0.5672 - lr: 0.0010\n",
            "Epoch 111/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.9207 - accuracy: 0.5788\n",
            "Epoch 111: loss improved from 0.94040 to 0.92069, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 0.9207 - accuracy: 0.5788 - lr: 0.0010\n",
            "Epoch 112/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.9335 - accuracy: 0.5719\n",
            "Epoch 112: loss did not improve from 0.92069\n",
            "61/61 [==============================] - 13s 208ms/step - loss: 0.9335 - accuracy: 0.5719 - lr: 0.0010\n",
            "Epoch 113/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.9448 - accuracy: 0.5698\n",
            "Epoch 113: loss did not improve from 0.92069\n",
            "61/61 [==============================] - 13s 209ms/step - loss: 0.9448 - accuracy: 0.5698 - lr: 0.0010\n",
            "Epoch 114/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.9464 - accuracy: 0.5708\n",
            "Epoch 114: loss did not improve from 0.92069\n",
            "\n",
            "Epoch 114: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "61/61 [==============================] - 13s 209ms/step - loss: 0.9464 - accuracy: 0.5708 - lr: 0.0010\n",
            "Epoch 115/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7402 - accuracy: 0.6300\n",
            "Epoch 115: loss improved from 0.92069 to 0.74024, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 0.7402 - accuracy: 0.6300 - lr: 2.0000e-04\n",
            "Epoch 116/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6907 - accuracy: 0.6192\n",
            "Epoch 116: loss improved from 0.74024 to 0.69072, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 219ms/step - loss: 0.6907 - accuracy: 0.6192 - lr: 2.0000e-04\n",
            "Epoch 117/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6752 - accuracy: 0.6056\n",
            "Epoch 117: loss improved from 0.69072 to 0.67515, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 0.6752 - accuracy: 0.6056 - lr: 2.0000e-04\n",
            "Epoch 118/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6671 - accuracy: 0.5919\n",
            "Epoch 118: loss improved from 0.67515 to 0.66709, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 0.6671 - accuracy: 0.5919 - lr: 2.0000e-04\n",
            "Epoch 119/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6637 - accuracy: 0.5847\n",
            "Epoch 119: loss improved from 0.66709 to 0.66374, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 0.6637 - accuracy: 0.5847 - lr: 2.0000e-04\n",
            "Epoch 120/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6596 - accuracy: 0.5840\n",
            "Epoch 120: loss improved from 0.66374 to 0.65958, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 0.6596 - accuracy: 0.5840 - lr: 2.0000e-04\n",
            "Epoch 121/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6595 - accuracy: 0.5837\n",
            "Epoch 121: loss improved from 0.65958 to 0.65946, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 228ms/step - loss: 0.6595 - accuracy: 0.5837 - lr: 2.0000e-04\n",
            "Epoch 122/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6574 - accuracy: 0.5780\n",
            "Epoch 122: loss improved from 0.65946 to 0.65737, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 261ms/step - loss: 0.6574 - accuracy: 0.5780 - lr: 2.0000e-04\n",
            "Epoch 123/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6561 - accuracy: 0.5827\n",
            "Epoch 123: loss improved from 0.65737 to 0.65612, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 227ms/step - loss: 0.6561 - accuracy: 0.5827 - lr: 2.0000e-04\n",
            "Epoch 124/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6561 - accuracy: 0.5744\n",
            "Epoch 124: loss improved from 0.65612 to 0.65608, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 228ms/step - loss: 0.6561 - accuracy: 0.5744 - lr: 2.0000e-04\n",
            "Epoch 125/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6560 - accuracy: 0.5750\n",
            "Epoch 125: loss improved from 0.65608 to 0.65599, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 0.6560 - accuracy: 0.5750 - lr: 2.0000e-04\n",
            "Epoch 126/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6564 - accuracy: 0.5775\n",
            "Epoch 126: loss did not improve from 0.65599\n",
            "61/61 [==============================] - 13s 213ms/step - loss: 0.6564 - accuracy: 0.5775 - lr: 2.0000e-04\n",
            "Epoch 127/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6562 - accuracy: 0.5765\n",
            "Epoch 127: loss did not improve from 0.65599\n",
            "61/61 [==============================] - 13s 213ms/step - loss: 0.6562 - accuracy: 0.5765 - lr: 2.0000e-04\n",
            "Epoch 128/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6580 - accuracy: 0.5693\n",
            "Epoch 128: loss did not improve from 0.65599\n",
            "\n",
            "Epoch 128: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "61/61 [==============================] - 13s 212ms/step - loss: 0.6580 - accuracy: 0.5693 - lr: 2.0000e-04\n",
            "Epoch 129/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6233 - accuracy: 0.5896\n",
            "Epoch 129: loss improved from 0.65599 to 0.62327, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 0.6233 - accuracy: 0.5896 - lr: 1.0000e-04\n",
            "Epoch 130/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6212 - accuracy: 0.5865\n",
            "Epoch 130: loss improved from 0.62327 to 0.62118, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 0.6212 - accuracy: 0.5865 - lr: 1.0000e-04\n",
            "Epoch 131/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6204 - accuracy: 0.5827\n",
            "Epoch 131: loss improved from 0.62118 to 0.62044, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 0.6204 - accuracy: 0.5827 - lr: 1.0000e-04\n",
            "Epoch 132/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6209 - accuracy: 0.5796\n",
            "Epoch 132: loss did not improve from 0.62044\n",
            "61/61 [==============================] - 13s 214ms/step - loss: 0.6209 - accuracy: 0.5796 - lr: 1.0000e-04\n",
            "Epoch 133/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6202 - accuracy: 0.5714\n",
            "Epoch 133: loss improved from 0.62044 to 0.62021, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 221ms/step - loss: 0.6202 - accuracy: 0.5714 - lr: 1.0000e-04\n",
            "Epoch 134/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6200 - accuracy: 0.5791\n",
            "Epoch 134: loss improved from 0.62021 to 0.62000, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 219ms/step - loss: 0.6200 - accuracy: 0.5791 - lr: 1.0000e-04\n",
            "Epoch 135/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6193 - accuracy: 0.5775\n",
            "Epoch 135: loss improved from 0.62000 to 0.61934, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 0.6193 - accuracy: 0.5775 - lr: 1.0000e-04\n",
            "Epoch 136/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6190 - accuracy: 0.5747\n",
            "Epoch 136: loss improved from 0.61934 to 0.61898, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 231ms/step - loss: 0.6190 - accuracy: 0.5747 - lr: 1.0000e-04\n",
            "Epoch 137/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6192 - accuracy: 0.5801\n",
            "Epoch 137: loss did not improve from 0.61898\n",
            "61/61 [==============================] - 13s 213ms/step - loss: 0.6192 - accuracy: 0.5801 - lr: 1.0000e-04\n",
            "Epoch 138/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6197 - accuracy: 0.5798\n",
            "Epoch 138: loss did not improve from 0.61898\n",
            "61/61 [==============================] - 13s 212ms/step - loss: 0.6197 - accuracy: 0.5798 - lr: 1.0000e-04\n",
            "Epoch 139/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6194 - accuracy: 0.5719\n",
            "Epoch 139: loss did not improve from 0.61898\n",
            "61/61 [==============================] - 13s 214ms/step - loss: 0.6194 - accuracy: 0.5719 - lr: 1.0000e-04\n",
            "Epoch 140/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6192 - accuracy: 0.5780\n",
            "Epoch 140: loss did not improve from 0.61898\n",
            "61/61 [==============================] - 13s 214ms/step - loss: 0.6192 - accuracy: 0.5780 - lr: 1.0000e-04\n",
            "Epoch 141/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6178 - accuracy: 0.5775\n",
            "Epoch 141: loss improved from 0.61898 to 0.61784, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 221ms/step - loss: 0.6178 - accuracy: 0.5775 - lr: 1.0000e-04\n",
            "Epoch 142/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6184 - accuracy: 0.5770\n",
            "Epoch 142: loss did not improve from 0.61784\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 0.6184 - accuracy: 0.5770 - lr: 1.0000e-04\n",
            "Epoch 143/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6181 - accuracy: 0.5796\n",
            "Epoch 143: loss did not improve from 0.61784\n",
            "61/61 [==============================] - 13s 214ms/step - loss: 0.6181 - accuracy: 0.5796 - lr: 1.0000e-04\n",
            "Epoch 144/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6185 - accuracy: 0.5685\n",
            "Epoch 144: loss did not improve from 0.61784\n",
            "61/61 [==============================] - 13s 213ms/step - loss: 0.6185 - accuracy: 0.5685 - lr: 1.0000e-04\n",
            "Epoch 145/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6180 - accuracy: 0.5744\n",
            "Epoch 145: loss did not improve from 0.61784\n",
            "61/61 [==============================] - 13s 212ms/step - loss: 0.6180 - accuracy: 0.5744 - lr: 1.0000e-04\n",
            "Epoch 146/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6177 - accuracy: 0.5739\n",
            "Epoch 146: loss improved from 0.61784 to 0.61772, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 0.6177 - accuracy: 0.5739 - lr: 1.0000e-04\n",
            "Epoch 147/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6171 - accuracy: 0.5832\n",
            "Epoch 147: loss improved from 0.61772 to 0.61710, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 0.6171 - accuracy: 0.5832 - lr: 1.0000e-04\n",
            "Epoch 148/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6175 - accuracy: 0.5698\n",
            "Epoch 148: loss did not improve from 0.61710\n",
            "61/61 [==============================] - 13s 214ms/step - loss: 0.6175 - accuracy: 0.5698 - lr: 1.0000e-04\n",
            "Epoch 149/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6178 - accuracy: 0.5804\n",
            "Epoch 149: loss did not improve from 0.61710\n",
            "61/61 [==============================] - 13s 215ms/step - loss: 0.6178 - accuracy: 0.5804 - lr: 1.0000e-04\n",
            "Epoch 150/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6184 - accuracy: 0.5775\n",
            "Epoch 150: loss did not improve from 0.61710\n",
            "61/61 [==============================] - 13s 214ms/step - loss: 0.6184 - accuracy: 0.5775 - lr: 1.0000e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd63dc87150>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "model.fit(X, y, epochs=150, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1DCpawtR-qZ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Prdiction part will continue on 2.O notebook"
      ],
      "metadata": {
        "id": "44lOxKzS__w1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9YP39LC7kKaz"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "O8yCMB-b_0Rc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Next_word_nlp.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMxd72u/QEVB9nrd/qGvOI7"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}